{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Some Installs"
      ],
      "metadata": {
        "id": "oR2Gi2-9aaD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  -q -U langgraph \"langchain[openai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDQQT3U3ZXyN",
        "outputId": "39775520-0098-40af-c53e-b03d424efde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.5/449.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q gradio"
      ],
      "metadata": {
        "id": "v873W4HzaG_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgSkU4XbaKxg",
        "outputId": "0bdc98e0-1934-4e7b-dbe8-f55fe8bffe15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wikipedia"
      ],
      "metadata": {
        "id": "jnuVd7OxW9Z7",
        "outputId": "46130d21-4895-4be1-c788-c56d97571c41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iPfM5a5GaeLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Annotated, TypedDict, List, Dict, Any, Optional\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
        "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import ToolNode,tools_condition\n",
        "from langgraph.graph.message import add_messages\n",
        "from pydantic import BaseModel, Field\n",
        "from IPython.display import Image, display\n",
        "import gradio as gr\n",
        "import uuid\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from langchain.agents import tool,Tool"
      ],
      "metadata": {
        "id": "67gmWPQ-ZWuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Start"
      ],
      "metadata": {
        "id": "I2CUR5tAaix6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5QxHi9VYuf7"
      },
      "outputs": [],
      "source": [
        "# LangSmith\n",
        "f=open(\"/content/LangSmith API.txt\",\"r\")\n",
        "smith_api_key=f.read()\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = smith_api_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"My-Research-Assistant\"\n",
        "\n",
        "# OpenAI Key\n",
        "f=open(\"/content/OpenAIKey.txt\",\"r\")\n",
        "api_key=f.read()\n",
        "\n",
        "# Serper API\n",
        "f=open(\"/content/Serper APIkey.txt\",\"r\")\n",
        "S_api_key=f.read()\n",
        "os.environ[\"SERPER_API_KEY\"] = S_api_key\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The LLM I Will Use"
      ],
      "metadata": {
        "id": "tfiDukAYa6wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model_name=\"x-ai/grok-4-fast:free\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.0)"
      ],
      "metadata": {
        "id": "aLGIT2KAa5v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I Will Create A Set Of Researchers"
      ],
      "metadata": {
        "id": "0D73ohLG2WyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Researchers(BaseModel):\n",
        "  name: str= Field(description='The name of the researcher')\n",
        "  role: str= Field(description='The role of the researcher in the context of the topic')\n",
        "  research_interests: str= Field(description='The research interests of the researcher')\n",
        "  CV: str= Field(description='A sort cv information about the researcher, one line max.')\n",
        "\n",
        "  @property\n",
        "  def description(self)->str:\n",
        "    return f\"Name: {self.name}\\nRole: {self.role}\\nResearch Interests: {self.research_interests}\\nCV Information: {self.CV}\""
      ],
      "metadata": {
        "id": "sBbJmryNcPTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Res_List(BaseModel):\n",
        "  re_list: List[Researchers]= Field(description='The list of researchers with role, name, research interests and research field')"
      ],
      "metadata": {
        "id": "2O1K6tZd38cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Re_State(TypedDict):\n",
        "  re_list: List[Researchers]\n",
        "  topic: str\n",
        "  max_researchers:int\n",
        "\n"
      ],
      "metadata": {
        "id": "7o-Fsx2q4Zlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_researchers(state:Re_State)->Res_List:\n",
        "  llm_with_structure=llm.with_structured_output(Res_List)\n",
        "\n",
        "  prompt=ChatPromptTemplate.from_messages([\n",
        "      ('system',\"\"\"You are tasked with creating a set of AI researchers.\n",
        "      1. First read carefully the the topic:\\n {topic}\n",
        "      2. Determine the most interesting themes based upon documents.\n",
        "      3. Pick the top {max_researchers} themes.\"\"\"),\n",
        "      ('user',\" Generate the set of researchers\")])\n",
        "\n",
        "  output=llm_with_structure.invoke(prompt.format_prompt(topic=state['topic'],max_researchers=state['max_researchers']).to_messages())\n",
        "\n",
        "  return {'re_list':output.re_list}"
      ],
      "metadata": {
        "id": "Y3cFBzvf5XHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder=StateGraph(Re_State)\n",
        "builder.add_node('create_researchers',create_researchers)\n",
        "\n",
        "builder.add_edge(START,'create_researchers')\n",
        "builder.add_edge('create_researchers',END)"
      ],
      "metadata": {
        "id": "ZzDhP7jG8ZQQ",
        "outputId": "c8797f94-3720-42e6-b20b-68b8d0850acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7c1250aa7470>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph=builder.compile()"
      ],
      "metadata": {
        "id": "7UXLgI1r-VpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "5iMZLjV6-Rdv",
        "outputId": "17f38d58-8285-431f-c6c3-bbea66085cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADqCAIAAACeFxhsAAAQAElEQVR4nOydCXwTRfvHZzdp0vRIT3rQltJytxQBy+UBVgr+BX0BUZBbkFMFAfHgUAEBlcsLRXlBEBEFAVEBuUE5lRvKXWih0NL7Sts0aXb/T7IlTbeTkPo2uiHPt/3ks5mZnZnM/nbmmdnZGTnP8wRB7oWcIIgdoFAQu0ChIHaBQkHsAoWC2AUKBbELpxTKuUMFNy+WlBRXVOh4vc7owrIMx/EMw0Bvn2XBAb7CJ8+wxg9hCAB84YvpD45J1bAAY/TiOd4YCW/0ZljjVyEMxMBzpkCm2EzHhGUIxxsjgX9IiIXTITBXNdAg5Mciy4zcjbgpGXWgvFGcV5PWauJsME40jrJ33Z3U86WlGk4mg0Jn3dwZGVyPCriEdy8nyIITLnzlFTUKBS43x5gi4O8KBEIYAzCCM2PyAmGAPsDVeFAVW+WxUTJMlbwYY3jjiYwpRdbkw1nkVciJGciBnKmo4CvKOfiEkJ5qWfMO3h2eCCROgnMIZce3GdfPlcjdmNAo9w49/OvVVxFnJuWi5tSe/Ky0cpDeA4/6dOzpBHJxAqEsm34dbvWOT/q1fNif3F8c2Jx5/nCx0oMdPjOaSBtJC+XMwfwDG3Obt/dMHBBK7l82fpZ2J7X85UWNiYSRrlA0BbpVs2+OWxAlA5Pkfufi8cI932WPWyjdHytRoZzYk/vnb/kvLZT0TVbnLJmcPGp2hNJLSaQHS6RHUZ7u6DaXUwnwxJCgFTPTiCSRolDWfnizTYIPcT2atFGHRCtXzrpOpIfkhLLhkzSFO/vQU/WIS/LMSxHlpfzhX7OJxJCcUMD+/8/YYOLCPNBZffZgIZEY0hLKpiW3lB5MYKgncWE69azHGcif23OJlJCWUDJvaFs+5IrWiYiQaPekw9KqVCQklFtXSwwVcD/9o+PZ165de+qpp0jtWb9+/bvvvkscQ6ee/loNR6SEhIRy+o8CaHfIP8uFCxfI3+Jvn2gPoZEerJycPZBHJIOEphkUZOq9fB2Vn+Li4i+//PLgwYN5eXkxMTFPPvlk7969wWX58uXgGx8fP2nSpEGDBh04cGDHjh2nTp0qLCxs2bLlyJEjwQsCJCcnP//88x9//PGcOXP8/Py8vb1PnjwJ7lu3bl2zZk3z5s1JXQOPx29e0bZ6lEgECQmlvIwLDnLUoOSsWbMyMzOnTp0aFRUFrcb7778fHR09duxYnU63c+fOLVu2QBitVjtjxoz27dtDYPi6e/duUM/mzZsDAgLc3NzABVQ1ZMiQ1q1bx8bGvvDCC5GRkUJIR+DuwRbn64lkkJBQOAPvrnLUkw6oAIYOHdqxY0c4Hj9+fGJioq+vryiMu7v7Dz/8oFKpBC+oUTZs2HD69OmuXbuaJj0ROB1qHfKPoFDIdHoDkQySmuHGEtZRD56gGoA2oqCgoG3btp06dWrRogU1WElJyZIlS06cOJGTkyO45Ofnm32tneUIOJ7jDf+0xWYDCRmzrJzXaR0llJkzZw4cOPDIkSOTJ0/u1q3b0qVLKyoqRGHu3LkDRoler583bx6EPHr0qCiAUvnPPa6r0POslO5iCeVF7sYU5VQQx6BWq0eMGDF8+PAzZ87s27dvxYoVYJAOHjzYMsyuXbvAZAGzA1ofUr0u+efRlhgCw9yJZJCQUHzruWWn6YgDgC7M9u3be/XqBVZIaxOXL1++dOlSzWCgJ0ElwJ49e8i/h76cD2siofkGEmp6mrfz1pY5ZJRJLpcvW7bszTffhOokNzcX+rSgEpALeDVo0ADMkf3799+4caNJkyZwvHHjRmiVDh8+/Ndff4FVC+0RNc6IiIikpKRjx45Bf5vUNZoCHYw9tkuU0FxaGTTeRBoE1nc/tjPP3YsNblDHVa5CoYiLi4OWZeXKlWDSpqWljRo1CsZRoC8TGBgIQ2erVq0CTfTv399gMKxdu/bTTz+Fdmf69OmlpaXffvstqKdVq1br1q3r0aNHeHi4ECeMpsCgy/fff9+hQwezY12x49uMkqKK+EQJzRGW1gy3Ne/f0JVxI2ZHEdfmiynJTVp7dRscQiSDtB4K9pscVlosocGDf4XzRwrh6bGkVEKk9qagQin3DXaDemXw1EhqADAmrLWVPj4+YI1SvaCVmThxInEMEDMMypFaZgn6Vl26dKF6/b4pOzJWQv0dASlOrl4yOfnJEcGNWnrX9AIzs6ysjHoWjH8IA+01AXfo7xDHAHYMWDakllmCvhWY2DXdd39/59oZzZgPJDdfWIrvHj/ay2/nqsxxCylCgcKF8Q8iJTw8PEjdcemYZtwCKZpoUpxc/UCXgIgWHivevkZcjKVvJMNNIs1Xe6T7AtjFYwX71uW4zksbSyYlPzcpLLiBRF+rlvQrpVu/Tk+7VJo4OKhxK+dbJ8J+Dm7OPv17YUI//9hO0n25WuovqScdyT+wKdcn0G3gm5HkviPjZtn2rzO0pdywaREevlJ8QdCMcyx78f38m7kZOp9AWdyj3q07O82aIjY49HPW5ZMlZcWG+o3d+7xUxwO7jsCZFtJZv+hGbqae54hSxXr6yFXerNJdxvHVJm0IayERizWVhNWTLMPIWMbAiX81WJA1O7mMabEcjlpCvLACT7W0TEs6VXOpDECIQc/pdBWaAgM8FtZpibDWS6+xYcRJcCahCKQkaS4dL8rP1Ou0nF7HV+iq5d98qWy4sDKGM9QQipwxVBgdeeP6SBwrLPBlXHOLJ7QnlRzh2btKEcmCnirhlR6Mwl0GT7LaJPgEOttaQM4nFEcDjwAHDRq0Y8cOgliAq0KKgcFf6pipi4MlIgaFQgVLRAwKhQqWiBgUChUsETEoFCpYImJQKFSwRMSgUKhgiYixMdvIlUGhiMEahQqWiBgUChUsETEoFCpYImJQKFSwRMSgMUsFhSIGaxQqWCJiUChUsETEoFCoYImIQRuFCgpFDNYoVLBExKBQqGCJiEGhUJHiu8f/LigUKigUMWjMUsFbRwzWKFSwRMR4eHg4btUd5wWFIkar1ZaWlhKkOigUMdDu1Fz9HEGhiEGhUEGhiEGhUMHusRgUChWsUcSgUKigUMSgUKigUMSgUKigUMSgUKigUMTIZDJrS5a7MigUMVijUEGhiEGhUEGhiEGhUEGhiEGhUEGhiEGhUEGhiEGhUEGhiAGhYPe4JrhydSXDhg07e/ascU1znodPwRGOT506RRB8emxmwoQJQUFBLMvCgBtrAlTy4IMPEsQECqUS0ERMTIyli7e396BBgwhiAoVSxciRI/38/Mxfo6KiEhISCGIChVJFbGysua1RKBRYnViCQqnGiy++GBwcDAcNGzbs3r07Qe5y717PzSslV08Wl2spXubttoiwB5IpKqHHIIpV2PpIxhKDaJMk0wZIVTtoCV0O81fRhkkWgcVbbFl+rb6pUs1dl4QIRF7m/F+4cD49Pb1FTEx4WJjZl2UYriqWys2/bCRaM+nKY1owcwDwqm0XlLKnVGWGxfueEYsfbgauiEJF2nb18vH3IrYTsi2UFe8kl5cSNyWrL6cEY1jCc+ac3d2jDQqVJ1z1aFmW4Ti+5nZslVfdIh5iufMazZ1lCcfVEIqpYEilTI1yq+lVFTPkjxdfaSHaSkxKrO5rzP/dSCrjp8QsHNM2jLN2Rc0nwI+FXNm6HNVvKmvRVpYSQ9nhrqYjK+NZGavXcT5+7ODp0cQ6tnL21VvJgWHy7kMbEuR+Z8NnyW4y+eCpDa0FsCqU/05PDm/i/kgfJ9hAE6kTfv0q1aDnh0yPovrSjdkjW7I4A0GVuBRPj2lYlGvIztZQfelCuXlV6+6Nj4FcDoU7c3ZvCdWLrgZ9KUfdwxW5z+GZsiL6A1G6UKATy3MMQVyMCgPPV9CvO7YviF3QhQIjBLUe+kGcH+NIjZWGxNoQPsrEFREGI6nQaxQYD8X5TIglaKMgdmHNRsHGxxUxPimzYoxYaXqMT6cI4moYL7qV8TPW+hkIUoWVpochBMfbXA/j3IpaNT3E/MIC4lpYbUro+kGVuCa1tlE4jufQSvkHuXXrZkLX+GPHjxKpYqVGYaRVqaSkXHt+4FME+few1j2W1jDK5SsXCPKvUmcjswaD4ccN332zehkcx7SIe2HYmLi41nDcq0/XoYNH/nFw79mzp37evFftrd6+49dfft2YkpIcFdX48YTufZ8ZIJjOGo3mxw1r/jp2JDX1WoB/4EMPdRkxfJy7u/vKVV+u/nY5BIDK+aVxk557dtD582choUuXzvv4+nXq+OiwoaM9PT1tZ+/dmW/IZLLg4NAf1q2eNXN+50cfz8vL/WLp4qTzZ7Rabbt2nSCTERGRxDQze+Om73fs2JJ260Zkg6j4+I6QDTgXvGyku+mndUePHrh4MUmhVD7Qqu2LL74cVt84P3Djph/Wfr9y0sSpkIHevfuNf3lKUXHRV199su23n318fOMf7DBq5Pjg4BBzPhctnrtl608BAYGQwwnj3xAcraVbM/Kjfx5at271pcvn/f0DW7Z8YPTI8RAVsRsbA250Z5ncajfJGsv++9nPP/84e9bCGdPm1qsX/ObU8TdvpoK7m5vblm0/NW7cbMH8zz1UHrv3bP9w/qymTZqvXfPLyBdf3rBx7ZIvFgkxbPoJfvaq/v2GzJv78Zgxr+7/fZcgu+EvjH2+/1AozX17joNKbt1Om/LGS9py7ZLPVr43a+H161cnTR59z4UqIBvXU5Lhf+57i1vFtQFZT3ptzOkzJyZNnPb18nV+vv4vvTzsdvotYzY2/bDmu6+f7Tvwh7Vbnn6679Ztm0Fb4G4j3XPnTn+2ZEFs7AOzZy98681Z+fl5c+fNENJVKBSlpSW//LJh6luz+/TqB+HfmjohJzd78aIvx7/yelZ25lvTJpgzD7dEq1Ztwavfc4N/2rx+776dttMVRX7l6qWp015t06bdqq83gMiuXbvy4fyZpDbYMGatTFyq4Gs1camwqHD9j2smvvpWu/iO8LVDh4fhB+Tm5TRo0BBqC7XaB8QuhNy2bXOrVm0gJBz7+fkPHzZ2/sLZgweOgGMonS6du0ZGVk7uTUo689exw2NGTxCltXv3b25yNygyuCPh65TX3h4w6OmDh/Y/1iXRRg4hG3fupH/5xbfCXjynT58AHS9auLRtm3bwddzYiYcO/75x41oo3zNnTzZrFvPEE0aT6KmefaDcy0y7sthINyYmbuWK9eHhDYQdoSr0+mkzJkGZ+Kh9IF2osZ5/fpiQEISHWueblRugZOAr1GFQblC3CZls0zq+W+KTwgHcNufOnYIa10a6oshB4vDrBg8awbIs3FfNm8XAjUHqiLppelJTrsFn8+axlZHK5bNnLTD7Nmta+fI3x3FQ1Q8dMsrsBZcBHM+eOwUSgZv+2PEjH3z4bvK1K8IdA+qpmdb582cgIaHUgJCQ0Pr1wyEG20IBoB0x79h0Luk0JCeULzHJqPUDD4JE4BhqbKgd5y+YDYLu1Kmz0ILYThcapvT0W59/iEILwAAADmtJREFUsejipaSSkso5pwX5eSAU4bh5s8qSuXbtqoeHh6ASAGrWGdPmEFOvBz7jWrY259ZH7VteXm7P7zVH3jKuNehm6vSJ0KJBzsPDIkBwpI6gC4WVE4Oe2I9GUwyf7kr6xllQQwoHOp1Or9ev+PoL+LcMAHU1MTVeUN9Ao9MuvhPcEMtXfA4NOTWtS5cvgL1SLYa7N6UNwHqwjARyIorE19f4hjo0Oh4enlDBQBMJin/ssW5jRk0IDKxnI91Dh36f8c5rgwYOHzP61UaNmhw/8ecbb75CaCVQUqJRKq1uLyajbVF3z99rjhxk98H7n/7xxx4oyS+WfvRg2/ZgKYLuid3U+qEgV1G7p8eensYXEqG5sR0Mbmi4n7p369m5c1dL9/qh4WBC/rplI1wkqO0FR0F8NfEPCAQzGQwXS0e4/0htABNPpVLNnfORpaOMNVqsUG9DHuA/NfX6yZN/rVq9DK7uvDkf2UgXjDDwApPLds6JcR86z7KyUqhEIRViH7X6vR3aPwT/EPjEiT/BJJ82feKmjbvs3yLR2Nut1cQl4xuUtREK2KqQG6i6W7RoaUqPhwowoUs3oaW3pFGjpsWaYnOVCLd1RsbtoKBgOCgrKwsMDBLcoe45fOQPalqNopvs3LUVehbmsoYrCvYBqQ2QDUguKCjE3LKkZ9z29THWKNDfadq0RVRUo4YNo+Efcrt120+20y0qKgwJDjVHfuDAXmvpgt0ArcPlKxdbmJppsJMWfzxv/MuvKy1qu7/9e8HwKteVg1Cg/oOSDwmpP3Hy6DuZGdAGEbuxNsPN6shsrZ4fe3l5dUvsAb2e37b/cur0cegCgKIF0YgY9eIrhw7thzYF7iroLMx+b+rkKWNBFlB/QssNp0PXo7CwACxcaLCLi4uEJh/KJTc35+DB/WlpN559dhCcC30lKHH4+tWyT0eM7F9bqw2q5fbtH1q48L3MzDuQ3Oaffxw7bsj27b+A156929+Z+frhw3+ANXr06MEDB/e2jDXW3jbSbdyoKQyqwg8H0wrGCIQk4ArVTBc622FhEcuWfXrg4D445eNPPsjOyjTb71Ts/71g/82c9cavWzYVFORfuJgE5jAoxlLB/wt1tuzFqxPebN06HoYBJr821qiAmQvMJpslUIsu+/I7GFPp07cb9PqgVp/z3mLhfnp7+jywcl4Y/uzgob3hQo4c+Qp87dM3MeNOescOj4Bu3n53yp69O2AkZsXydSp31Zhxg4e+0Be6uK9PeRuaZ1JL3p/7cZcuibPnTO39TCKUaWLik8888zy4vzZ5RsPI6OlvT+7dp+uCRe89/FCXyZOmg7uNdEeMeAnu4xlvT+7+f51AedBDhpoDusEwFiBKFOrdhfO/gAck77z7Otgx7irV+/M+sd002P97odvYs0efJZ8vhLKFLjQ0cx8tXlZXW/PS3z1ePTeVNzDPvBpJEFdizdxr4VEeT4+jVEI4uRqpota9ntoas1IAzOekc6epXj169IYhNYLci1qPzPJ8tbVonIIpk2fo9DqqFzw6IMj/htUaxemanlo9/UKoGNfKstI/prdIBgPP42oGroexGeFr85I6vteDiMBeD1KFcWJQreajgI2C0/BdEOOgWu16PQRBqkGvUfhaPutB7nvQmEXswsoTIx6FglTDOV7XQP516EJRqGR8Be6r53IolIzMygwqujGr8iRaLQrF5dCVc4FhblQvulAS+gWWabDxcS1Skgrhs/0T9EdmdKH4BKhCohTfvV9nb4Ug0ufQz9lxj6it+drahuXo9uxTewtDoz3CmqhUHgpiB+a9bIitPYyIeW8kWwjbN9WKWiZpudFRzVNrboNkNdnq+wpVy7iVLNXIjLUCqXSnZb6yb1rTnaPGRc0JT4qLytMua3Jv6f7zUv2wKKvzMe6xsRNo5eJRjbbUYO9rPnYIwE7sicm2lu6ptHskYd1bFLNodyUb6f4N8VvNHV+56ARPc7cTCClzIyov5pG+9RrFqG2FxCFYEbm5uQMGDNi5cydBLMB1ZsVUVFTU1cz1+wksETEoFCpYImJQKFSwRMTo9Xo3NzeCVAeFIgZrFCpYImJQKFSwRMSgUKhgiYhBoVDBEhGDQqGCJSIGhUIFS0QMCoUKlogYFAoVLBExOOBGBYUiBmsUKlgiYlAoVLBExKBQqGCJiEEbhQoKRQzWKFSwRMSgUKhgiYhBoVCps5Wr7xtQKFRQKGLQmKWCt44YrFGoYImIUavVXl5eBKkOCkVMcXGxhweudC0GhSIG2p177nnqgqBQxKBQqKBQxMhkMoMBFxESg91jMVijUEGhiEGhUMGmRwwKhQoKRQwKhQoKRQwKhQoKRQwKhQoKRQwKhQoKRQwKhQoKRQwKhQoKRQwKhQoKRQwKhQoKRQw+66GCQhGDNQoVXLm6kj59+qSkpLAsa1zM3wQ4chx36tQpguBDQTPjxo1Tq9UgFGh6WNN2vqCYuLg4gphAoVTSvXv36OhoSxeVStW/f3+CmEChVDFs2DBvb2/z14iIiJ49exLEBAqlioSEhJiYGOEYGqC+ffsS5C4olGqYKxWoTnr16kWQuzh997ggR5efpaso58AK5QnPQD+O8Kzxswpbu3sxppOMAYzbMwW4x3aI/c/lq1e7P/J/aZf0DNGZNwETIq+ZAcYUA2/x1bwHGjH1m+QK4h8s9wlQEWfGKbvHJ3bnXDym0RRUVOiqud+96Mzd3buMP81yHy+eM20STwjFV9CY9R20qqmt2p5fIh1a3a9OrmDUAfL4br5N2/gQZ8PJhLJl+a2bl7SQY4WH3NPH3S/CW+XtTpyBwpzSwvTisgKtvpyTuzHN4r0SngsmzoPTCOX0/rzDW/IYlvGP9AmO9iPOTNqF7KIMjVzO9H01NDDUOd5KdA6hrPnwRkGWPqiRb1CUc0vEkrSkzML00ug4jx4j6hPJ4wRC+WbOjbISQ/POkeR+5MK+lMZxXt2HhBBpI3WhfDs3VVPEtXjs/lSJQNKulLBGyj4vRxAJI2mhfPNeql7PNO4UTu53Lu5LqR+t6jU2jEgV6Q647VqbUVpscAWVAC0SotKulF0+WUikinSFcvlYSXTHUOIy1ItW7/0+m0gViQpl9dxUlVqhVCmJyxDcKICRsb8su0UkiRSFotHoinIrGnWUboPtIKD/n3ZFSySJFIWyfcUdN6V0H0KdPrd7ytsdNCX5pK7xDzcO7f++KZNIDykKJSddrw520VXUVN7K62dLifSQnFCK83QVej60WQBxSeDpFfT1iPSQXA1/5kAh60j1pt48u3Pf8rRbF7w8/Vo0e6R7wkh3d09wP3T0x12/fz1uxNLVP0zNzLoeGty480MD2rV9Sjhry/bPjp/ZplR4tGn1RFBgA+Iw/EK9b5/LyUorDYqQVp0quRol+3Y5GP/EMeTkpn21arxeX/7K6OXDBn6YkXl16dfjDAbjyxkyuVtZWfHmrQv79Z62YPbRVi0fX795Tn7BHfA6/NfGw39teKbn66+OWRngV3/XvhXEoTDk+vkSIjEkJ5QyDceyDHEMJ89sl8vcXhjwYXC9hiFB0c/1mn4743LSxd8FX4NB3y1hZGREHMMw8a17wpj17Ywr4H7wyPpWsV1BOh4eaqhjGkfHE0fCytjCXD2RGJITikHPMw7LFbQ7EeExnp6+wld/v9AA//CUG6fNARqExQoHHio1fJZpi0EuOXlpwUFR5jDh9ZsTB2PQOepW+dtIzkZRKhgN6yhrrkyrSbt9ATq3lo5FxbnmY6bG5DZteQnHGZTKKotBoXDspEae5xRuKJR7IfdiuCxHCcXbOyAqsvUTj4+2dPT0tDUx0V3pybIyvb5qHKxc5+DuK0d8giS3vYfkhBIS7p6RXE4cQ/3gJifObItu2Ia927O6k3W9XoCtXgzUMX6+oak3z3V5uNLl4uVDxJHA4/xGbSQ3jCQ5GyU+0YfniIOAHi/Hcb/89pFOp83KvrFlx5JFSwZmZCbbPuuBlonnLuyDAVk43ntg9Y1bScRh5KQVsjLiX09yU/YlJxSFSiFXMOkXs4gDgG7LlFfWKtxUH385bP6n/a6nnnyu9/R7GqeJXYZ3eLDX5m2LwLiB6uQ/T04kxvveIfN4CjI0nj4yIj2kOHHpp89vZd3SNbtP5z7a5sLelLZdfDo+VY9IDCk+6+nzcrhey7ngajbZKfnQ25GgSohk3xT0C5JfO3y76aN0M7OgMGvhkgFUL5XSq6xcQ/UKqRf9yuj/krpjxtyu1rxgtFcmo5Rtg/DY0cM+tXZWTmphVEtPIkmkO2f289eSI9sGeflTCg4uQ2ER3YgBK1WhoL8SxrJyX58gUnfk5adb89LpyxVulFlXcrlC7R1IPSX9UnZxZsmYDxoRSSLdaR+tOvucPZAV2zWqphfcrP5+//67MHWbh7w0zbMTcXJ17Xm0Vz1ogC4fSCUuQNKulKZtPUMaSPdFdqm/17N+8a2cjPKYxxuS+5fzu1Oax6u7DqjLZrHOcYI3BdctTsvP1DV/rCG57zDoDZf/SIvtpO7SV4o9HUuc491jGFnJSNH6hHqFxUi9QO3n+on00tzyNo+pH+4l6bpEwGlWM0i9qNn+TSaM7nsHe4XHBBJnJvVkhiZXq1SRUXMbEyfBydZH2bs+8+pJTYWOl7vLlF4KdT137yBPN4XknrWK0JXqCrNKCzM15Ro9byDQcW73hF/bBGeaF+yUKy5dO1t0en9hbqZeV8ZVrm5UudaSGGurHzG08DUdq1bwMmO5gFOlS7UlmkSRsHdjYFgCI3AhkcoufQP9gp1vma77YeXq4nydttRAmKpnaWZ9MITlCUeqK4YRfjVTXUWmJYiFC212Y01KqVlAzN1gxji4u8owrdfF8gxnuYCcgXj6EQ8vBXFycIlzxC5w0wTELlAoiF2gUBC7QKEgdoFCQewChYLYxf8DAAD//6Ca9nMAAAAGSURBVAMA9u1cFc+Gl1UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state={'re_list':[],'topic':'AI','max_researchers':5}\n",
        "researchers=graph.invoke(state)"
      ],
      "metadata": {
        "id": "0yLoEIjy-a93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "researchers['re_list']"
      ],
      "metadata": {
        "id": "JOkVCYsD-pkB",
        "outputId": "781e9669-99b7-486c-bf56-2fee5c9988f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Researchers(name='Dr. Elena Vasquez', role='Lead Researcher in Machine Learning', research_interests='Deep learning algorithms, neural networks, and scalable AI models', CV='PhD in Computer Science from MIT, 10+ years at Google AI, published 50+ papers on arXiv.'),\n",
              " Researchers(name='Prof. Raj Patel', role='Expert in Natural Language Processing', research_interests='Large language models, sentiment analysis, and multilingual AI systems', CV='Professor at Stanford, former OpenAI researcher, authored key papers on transformers.'),\n",
              " Researchers(name='Dr. Sophia Chen', role='Specialist in Computer Vision', research_interests='Image recognition, object detection, and generative adversarial networks', CV='MS from UC Berkeley, 8 years at Meta AI, contributed to CVPR award-winning projects.'),\n",
              " Researchers(name='Dr. Marcus Lee', role='Pioneer in Reinforcement Learning', research_interests='Multi-agent systems, robotics applications, and decision-making AI', CV='PhD from Carnegie Mellon, led DeepMind team on AlphaGo, 20+ patents in RL.'),\n",
              " Researchers(name='Dr. Aisha Rahman', role='Advocate for AI Ethics and Safety', research_interests='Bias mitigation, AI governance, and responsible AI deployment', CV='JD from Harvard, researcher at Future of Life Institute, consulted for UN on AI policy.')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's see an example of our researchers"
      ],
      "metadata": {
        "id": "Ua3jV7rt_5zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for researcher in researchers['re_list']:\n",
        "  print(researcher.description)\n",
        "  print(\"...\"*20)"
      ],
      "metadata": {
        "id": "5VBtfLRX-1Wd",
        "outputId": "ad75a540-8359-4fd7-aeb5-de01daa1a32a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Dr. Elena Vasquez\n",
            "Role: Lead Researcher in Machine Learning\n",
            "Research Interests: Deep learning algorithms, neural networks, and scalable AI models\n",
            "CV Information: PhD in Computer Science from MIT, 10+ years at Google AI, published 50+ papers on arXiv.\n",
            "............................................................\n",
            "Name: Prof. Raj Patel\n",
            "Role: Expert in Natural Language Processing\n",
            "Research Interests: Large language models, sentiment analysis, and multilingual AI systems\n",
            "CV Information: Professor at Stanford, former OpenAI researcher, authored key papers on transformers.\n",
            "............................................................\n",
            "Name: Dr. Sophia Chen\n",
            "Role: Specialist in Computer Vision\n",
            "Research Interests: Image recognition, object detection, and generative adversarial networks\n",
            "CV Information: MS from UC Berkeley, 8 years at Meta AI, contributed to CVPR award-winning projects.\n",
            "............................................................\n",
            "Name: Dr. Marcus Lee\n",
            "Role: Pioneer in Reinforcement Learning\n",
            "Research Interests: Multi-agent systems, robotics applications, and decision-making AI\n",
            "CV Information: PhD from Carnegie Mellon, led DeepMind team on AlphaGo, 20+ patents in RL.\n",
            "............................................................\n",
            "Name: Dr. Aisha Rahman\n",
            "Role: Advocate for AI Ethics and Safety\n",
            "Research Interests: Bias mitigation, AI governance, and responsible AI deployment\n",
            "CV Information: JD from Harvard, researcher at Future of Life Institute, consulted for UN on AI policy.\n",
            "............................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hh_U5JAK_Ruy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I Want the above researchers to ask questions to an expert about a \"topic\""
      ],
      "metadata": {
        "id": "Edg3pvurCm0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "class QuestionState(TypedDict):\n",
        "  context : Annotated[list,operator.add]\n",
        "  questions_answer: Annotated[list,operator.add]\n",
        "  researcher: Researchers\n"
      ],
      "metadata": {
        "id": "cRoOj5A2UpdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_question(state:QuestionState)->str:\n",
        "\n",
        "  prompt=ChatPromptTemplate.from_messages([\n",
        "      ( \"system\", \"\"\"You are a researcher with the following profile:\\n\n",
        "       {self_info} \\n\n",
        "       Your role is to engage an expert in conversation about a topic.\n",
        "       - Ask precise, insightful questions that go beyond generalities.\n",
        "       - Aim to uncover surprising, scientifically grounded insights.\n",
        "       - Continuously refine your questions to drill deeper into the subject.\n",
        "       - Stay in character at all times, reflecting the profile and goals described above.\n",
        "       - When you are fulfilled with the expert's answer, you answer him: 'Thank you for your time, that helped me a lot.'  \"\"\" ),\n",
        "      ('user', 'Ask a question to the expert about {topic}') ])\n",
        "\n",
        "  question=llm.invoke(prompt.format_prompt(**{'self_info':state['researcher'].description,'topic':Re_State['topic']}).to_messages())\n",
        "\n",
        "  return {'questions_answers':f'question:{question.content}'}"
      ],
      "metadata": {
        "id": "8WkUC5E8XpMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's test it:\n",
        "Re_State['topic']:'About Ai'\n",
        "state={'context':[],'questions_answers':[],'researcher':researchers['re_list'][0]}\n",
        "make_question(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK7wyihhkI1f",
        "outputId": "0648defb-8d08-4c15-af84-7764ee45525e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'questions_answers': \"question:As Dr. Elena Vasquez, I'm always eager to dive into the nuances of scalable AI models. Given the topic of state management in dynamic environments like __main__.Re_State['topic'], could you elaborate on how such mechanisms handle concurrency issues in distributed neural network training, particularly when integrating with frameworks like PyTorch or TensorFlow? What unexpected challenges have you encountered in maintaining state consistency across nodes?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "serper_search = GoogleSerperAPIWrapper()\n",
        "from langchain_community.document_loaders import WikipediaLoader"
      ],
      "metadata": {
        "id": "CHTsWUK8SOGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I Run Some Test"
      ],
      "metadata": {
        "id": "EZx9O4ANVeeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs=WikipediaLoader('ai',load_max_docs=2).load()"
      ],
      "metadata": {
        "id": "aIl-uAlQWwg-",
        "outputId": "212167b1-91d3-4774-ec85-c78fcd5247f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata['summary']"
      ],
      "metadata": {
        "id": "T-9rYsgsXUeg",
        "outputId": "2cd991b0-7c4a-4e5f-eae0-cd6b7543adfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI\\'s ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI\\'s long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state={'context':[],'questions':[],'researcher':researchers['re_list'][0]}\n",
        "question=make_question(state)"
      ],
      "metadata": {
        "id": "FY95AM-UTclB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBWGHVALUL2w",
        "outputId": "aaa1cbbe-3693-4197-db5b-c899c12986ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': [\"As a fellow researcher in machine learning, I'm particularly intrigued by the evolution of scalable AI architectures. Could you elaborate on how recent advancements in federated learning are mitigating privacy concerns while enhancing model performance on heterogeneous datasets, and what unexpected trade-offs have you observed in real-world deployments?\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Create Our Search Functions"
      ],
      "metadata": {
        "id": "zvWQnTavVjdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchQuery(BaseModel):\n",
        "  search:str=Field(description='The search query')"
      ],
      "metadata": {
        "id": "KLLg4uWSUv8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_search(state:QuestionState):\n",
        "  prompt=ChatPromptTemplate.from_messages([\n",
        "      ('system',\"\"\"Given a conversation between a researcher and an expert on the {topic} your job is generate a query in order use it in a web-search related to that convertation.\n",
        "      1. Pay attention to the questions posed by the researcher.\n",
        "      2. Analyze the conversation carefully.\n",
        "      3. Convert researcher's final question into a web search query.\"\"\")])\n",
        "\n",
        "  search_llm_1=llm.with_structured_output(SearchQuery)\n",
        "\n",
        "  query=search_llm_1.invoke(prompt.format_prompt(topic=Re_State['topic']).to_messages())\n",
        "  search_output_1=serper_search.run(query.search)\n",
        "\n",
        "  return {'context':search_output_1}\n",
        "\n",
        "\n",
        "def wiki_search(state: QuestionState):\n",
        "  prompt=ChatPromptTemplate.from_messages([\n",
        "      ('system',\"\"\"Given a conversation between a researcher and an expert on the {topic} your job is generate a query in order use it in a wikipedia search related to that convertation.\n",
        "      1. Pay attention to the questions posed by the researcher.\n",
        "      2. Analyze the conversation carefully.\n",
        "      3. Convert researcher's final question into a wikipedia search query.\"\"\")])\n",
        "  search_llm_2=llm.with_structured_output(SearchQuery)\n",
        "\n",
        "  query=search_llm_2.invoke(prompt.format_prompt(topic=Re_State['topic']).to_messages())\n",
        "  docs=WikipediaLoader(query.search,load_max_docs=2).load()\n",
        "  search_output_2=docs[0].metadata['summary']\n",
        "\n",
        "  return {'context':search_output_2}"
      ],
      "metadata": {
        "id": "LJN_OhhmVPLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now It's Time For Our Expert"
      ],
      "metadata": {
        "id": "KUNvm9GEZFsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expert(state:QuestionState):\n",
        "  prompt=ChatPromptTemplate.from_messages([\n",
        "      ('system',\"\"\"\n",
        "# MISSION\n",
        "You are a senior expert in {topic}. Your role is to answer a researcher's question with precision, using **only** the provided context.\n",
        "\n",
        "# RESEARCHER PROFILE\n",
        "{researcher_info}\n",
        "\n",
        "# SOURCE CONTEXT\n",
        "{context}\n",
        "\n",
        "# INSTRUCTIONS\n",
        "1.  **Source Fidelity:** Your answer must be derived solely from the provided context. This is non-negotiable. If the answer isn't in the context, you must state that it is not covered.\n",
        "2.  **Tailor the Explanation:** Consider the researcher's background. If they are a specialist, use appropriate technical language. If they are a cross-disciplinary researcher, adjust the explanation to be accessible without losing scientific rigor.\n",
        "3.  **Response Structure:**\n",
        "    *   **Direct Answer:** Begin with a concise, direct answer to the question.\n",
        "    *   **Detailed Explanation:** Elaborate on the answer, citing specific details, data, or mechanisms from the context.\n",
        "    *   **Contextual Link:** Where relevant, connect the answer to the broader field or the researcher's specific interests as hinted in their profile.\n",
        "4.  **Maintain Scientific Integrity:** Present facts objectively. Differentiate between established findings (as per the context) and hypotheticals or suggested future directions (if mentioned in the context).\"\"\"),\n",
        "      ('user',\"{question}\")])\n",
        "\n",
        "  answer=llm.invoke(prompt.format_prompt(researcher_info=state['researcher'].description,\n",
        "                                         context=state['context'],\n",
        "                                         question=state['questions'][0]).to_messages())\n",
        "\n",
        "  return {'questions_answers':f'answer:{answer.content}'}\n",
        "\n"
      ],
      "metadata": {
        "id": "MMc93RtTWGtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writer(state: QuestionState):\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"\"\"You are a professional technical writer. Your task is to create a clear, well-structured report based on the provided documents.\n",
        "\n",
        "Documents to analyze:\\n\n",
        "{documents}\n",
        "\\n\n",
        "Follow these rules when writing the report:\n",
        "\n",
        "1. Format the report using **Markdown**:\n",
        "   - Use `##` for section titles.\n",
        "   - Use `###` for sub-section headers.\n",
        "\n",
        "2. The report must include the following sections:\n",
        "   - ## Title\n",
        "   - ### Summary\n",
        "   - ### Sources\n",
        "\n",
        "3. The report should be **concise, objective, and professional**, with a maximum length of **500 words**.\n",
        "\n",
        "4. Ensure the summary accurately reflects the main insights from the documents.\"\"\"\n",
        "    ),\n",
        "    (\n",
        "        \"user\",\n",
        "        \"Write a report based on the provided documents.\")])\n",
        "  answer=llm.invoke(prompt.format_prompt(documents=state['context']).to_messages())\n",
        "\n",
        "  return {'questions_answers':f'answer:{answer.content}'}\n"
      ],
      "metadata": {
        "id": "PJWuqXheIOzr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}